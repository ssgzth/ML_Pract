{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssgzth/ML_Pract/blob/main/Amazon_Sentimental_Analysis(Binary).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Sl2fmE7SuqBs"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dst-yKi4vETy"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('review.csv') # name of the csv file is 'review.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PO87hgGpvrnJ"
      },
      "outputs": [],
      "source": [
        "text = df['text'] #extracting the reviews\n",
        "label = df['score'] #extracting the ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "0KFs77TE2OkD",
        "outputId": "75495c4c-696c-47d2-df57-1e66f927368c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'edit after months of using this shoe i have decided to reduce my rating to stars i still like the shoes they fit great and grip fantastically however this grip i believe is provided in a trade off for durability the rubber while super grippy is soft and deteriorates rather quickly i spoke with a rep at rei about madrock shoes my wear issue and she said that madrock uses thinnercheaper rubber to sell at lower prices she said that most people who buy madrock have the intention of resoling with better rubber once they are broken in i would still recommend these shoes to anyone who is starting out but keep in mind they will wear down quickly end edit ive been climbing indoor for about two months and after getting acquainted with my abilities through rental shoes i decided i really needed a pair of my own to push routes i had been struggling on i like these for the following reasons comfort i tried on a ton of shoes in person to make sure i knew what i was getting it was a toss up between these and thela sportiva nago mens climbing shoe both provided a great fit however i felt that the mad rock phoenixs had less of a dead toe feel the being said these have a great toe box and firm rand enough to provide plenty of comfort when edging on tiny holds these also grip my heels extremely well so heel hooks and the like arent a problem i remove them during long rests but they are comfortable enough to leave on for extended periods of timefit street shoes i wear an and i like my climbing shoes tight not uncomfortably tight but tolerable i was wearing a in the rental evolv defys at my gym however i was surprised that i had to size down to a in these to get the feel i was looking for since these arent synthetic uppers i fully expect them to stretch a bit but as of now they are nice and tight the laceups really provide a wide range of fit and adjustability i figure that the best way to find a climbing shoe is to first try it on in person develop a relationship with that shoe and then for any future buys order it onlineperformance this is where having your own shoes really pays off before i got these i was doing and some routes after getting these i redid some of the routes that i struggled on before and practically ran up them im now completing bc routes and theres a im excited to work these shoes have a slight downturn which provides great tension for sporty overhang routes and enough edge to stand on tiny holds firmly the toe is thin enough to jam into finger cracks and the heel has a ridged grip which really grabs holds my dyno has increased roughly inches with these and i can slab up slippery faces pretty easily now the box has some label about sciencefriction rubber on the soles and i can attest to its abilitycare so far i havent had to do anything to these to prevent them from smelling as with any shoe you want to let them air out after each climb away from direct sun light additionally i have been climbing about days a week in these for almost weeks now and they dont show a sign of wear i imagine once i venture outdoors i hear austin tx climbing has it all great limestone deepwater soloing and pink granite a bit west they might start to wear but until then they seem well constructedoverall i would say this is a great shoe for indoor climbing having not ventured onto any real rock yet i cant speak toward its performance however i would imagine that it would function just fine on real rock as it does on whatever those indoor gyms are molded out of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42rUmBoFwdPU",
        "outputId": "f601f549-157e-4dcf-a0aa-d28fe7fcedc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26684"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p3G9UEH8wfuh"
      },
      "outputs": [],
      "source": [
        "#Tokenize â€” Create Vocab to Int mapping dictionary\n",
        "from collections import Counter\n",
        "all_text2 = ' '.join(text)\n",
        "# create a list of words\n",
        "words = all_text2.split()\n",
        "# Count all the words using Counter Method\n",
        "count_words = Counter(words)\n",
        "\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Puw2uUah1jgZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h20lwXK6w29j"
      },
      "outputs": [],
      "source": [
        "#In order to create a vocab to int mapping dictionary\n",
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACcD_izF197v",
        "outputId": "7d780d47-c0c1-48a2-8ae3-891e5705f522"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'i': 1,\n",
              " 'the': 2,\n",
              " 'and': 3,\n",
              " 'a': 4,\n",
              " 'to': 5,\n",
              " 'they': 6,\n",
              " 'for': 7,\n",
              " 'of': 8,\n",
              " 'them': 9,\n",
              " 'my': 10,\n",
              " 'in': 11,\n",
              " 'these': 12,\n",
              " 'are': 13,\n",
              " 'have': 14,\n",
              " 'shoes': 15,\n",
              " 'is': 16,\n",
              " 'on': 17,\n",
              " 'that': 18,\n",
              " 'it': 19,\n",
              " 'but': 20,\n",
              " 'with': 21,\n",
              " 'this': 22,\n",
              " 'pair': 23,\n",
              " 'so': 24,\n",
              " 'not': 25,\n",
              " 'very': 26,\n",
              " 'comfortable': 27,\n",
              " 'as': 28,\n",
              " 'shoe': 29,\n",
              " 'was': 30,\n",
              " 'you': 31,\n",
              " 'wear': 32,\n",
              " 'great': 33,\n",
              " 'bought': 34,\n",
              " 'size': 35,\n",
              " 'be': 36,\n",
              " 'feet': 37,\n",
              " 'had': 38,\n",
              " 'boots': 39,\n",
              " 'like': 40,\n",
              " 'at': 41,\n",
              " 'just': 42,\n",
              " 'love': 43,\n",
              " 'all': 44,\n",
              " 'were': 45,\n",
              " 'good': 46,\n",
              " 'would': 47,\n",
              " 'fit': 48,\n",
              " 'or': 49,\n",
              " 'when': 50,\n",
              " 'am': 51,\n",
              " 'will': 52,\n",
              " 'if': 53,\n",
              " 'me': 54,\n",
              " 'out': 55,\n",
              " 'well': 56,\n",
              " 'get': 57,\n",
              " 'from': 58,\n",
              " 'more': 59,\n",
              " 'than': 60,\n",
              " 'years': 61,\n",
              " 'look': 62,\n",
              " 'can': 63,\n",
              " 'time': 64,\n",
              " 'up': 65,\n",
              " 'one': 66,\n",
              " 'about': 67,\n",
              " 'because': 68,\n",
              " 'too': 69,\n",
              " 'buy': 70,\n",
              " 'after': 71,\n",
              " 'only': 72,\n",
              " 'got': 73,\n",
              " 'he': 74,\n",
              " 'your': 75,\n",
              " 'price': 76,\n",
              " 'dont': 77,\n",
              " 'little': 78,\n",
              " 'first': 79,\n",
              " 'other': 80,\n",
              " 'do': 81,\n",
              " 'been': 82,\n",
              " 'day': 83,\n",
              " 'no': 84,\n",
              " 'wearing': 85,\n",
              " 'really': 86,\n",
              " 'foot': 87,\n",
              " 'an': 88,\n",
              " 'also': 89,\n",
              " 'now': 90,\n",
              " 'has': 91,\n",
              " 'even': 92,\n",
              " 'made': 93,\n",
              " 'much': 94,\n",
              " 'quality': 95,\n",
              " 'its': 96,\n",
              " 'work': 97,\n",
              " 'leather': 98,\n",
              " 'im': 99,\n",
              " 'still': 100,\n",
              " 'which': 101,\n",
              " 'worn': 102,\n",
              " 'some': 103,\n",
              " 'boot': 104,\n",
              " 'what': 105,\n",
              " 'ive': 106,\n",
              " 'feel': 107,\n",
              " 'recommend': 108,\n",
              " 'new': 109,\n",
              " 'walking': 110,\n",
              " 'off': 111,\n",
              " 'most': 112,\n",
              " 'back': 113,\n",
              " 'find': 114,\n",
              " 'long': 115,\n",
              " 'ordered': 116,\n",
              " 'amazon': 117,\n",
              " 'last': 118,\n",
              " 'she': 119,\n",
              " 'another': 120,\n",
              " 'any': 121,\n",
              " 'around': 122,\n",
              " 'looking': 123,\n",
              " 'sandals': 124,\n",
              " 'over': 125,\n",
              " 'go': 126,\n",
              " 'nice': 127,\n",
              " 'color': 128,\n",
              " 'sole': 129,\n",
              " 'perfect': 130,\n",
              " 'use': 131,\n",
              " 'bit': 132,\n",
              " 'right': 133,\n",
              " 'wide': 134,\n",
              " 'black': 135,\n",
              " 'since': 136,\n",
              " 'did': 137,\n",
              " 'support': 138,\n",
              " 'never': 139,\n",
              " 'slippers': 140,\n",
              " 'could': 141,\n",
              " 'heel': 142,\n",
              " 'year': 143,\n",
              " 'better': 144,\n",
              " 'pairs': 145,\n",
              " 'every': 146,\n",
              " 'there': 147,\n",
              " 'two': 148,\n",
              " 'product': 149,\n",
              " 'style': 150,\n",
              " 'always': 151,\n",
              " 'by': 152,\n",
              " 'ever': 153,\n",
              " 'same': 154,\n",
              " 'best': 155,\n",
              " 'put': 156,\n",
              " 'found': 157,\n",
              " 'walk': 158,\n",
              " 'theyre': 159,\n",
              " 'order': 160,\n",
              " 'many': 161,\n",
              " 'big': 162,\n",
              " 'comfort': 163,\n",
              " 'how': 164,\n",
              " 'wore': 165,\n",
              " 'need': 166,\n",
              " 'enough': 167,\n",
              " 'make': 168,\n",
              " 'think': 169,\n",
              " 'easy': 170,\n",
              " 'again': 171,\n",
              " 'before': 172,\n",
              " 'small': 173,\n",
              " 'few': 174,\n",
              " 'then': 175,\n",
              " 'old': 176,\n",
              " 'run': 177,\n",
              " 'warm': 178,\n",
              " 'their': 179,\n",
              " 'happy': 180,\n",
              " 'keep': 181,\n",
              " 'while': 182,\n",
              " 'who': 183,\n",
              " 'toe': 184,\n",
              " 'way': 185,\n",
              " 'high': 186,\n",
              " 'lot': 187,\n",
              " 'purchased': 188,\n",
              " 'cant': 189,\n",
              " 'didnt': 190,\n",
              " 'used': 191,\n",
              " 'we': 192,\n",
              " 'though': 193,\n",
              " 'hard': 194,\n",
              " 'ago': 195,\n",
              " 'ones': 196,\n",
              " 'loves': 197,\n",
              " 'know': 198,\n",
              " 'months': 199,\n",
              " 'say': 200,\n",
              " 'without': 201,\n",
              " 'purchase': 202,\n",
              " 'true': 203,\n",
              " 'her': 204,\n",
              " 'down': 205,\n",
              " 'his': 206,\n",
              " 'problem': 207,\n",
              " 'going': 208,\n",
              " 'different': 209,\n",
              " 'however': 210,\n",
              " 'soles': 211,\n",
              " 'days': 212,\n",
              " 'socks': 213,\n",
              " 'buying': 214,\n",
              " 'narrow': 215,\n",
              " 'cute': 216,\n",
              " 'into': 217,\n",
              " 'take': 218,\n",
              " 'want': 219,\n",
              " 'pretty': 220,\n",
              " 'through': 221,\n",
              " 'soft': 222,\n",
              " 'several': 223,\n",
              " 'tried': 224,\n",
              " 'excellent': 225,\n",
              " 'looks': 226,\n",
              " 'half': 227,\n",
              " 'worth': 228,\n",
              " 'brand': 229,\n",
              " 'almost': 230,\n",
              " 'sure': 231,\n",
              " 'running': 232,\n",
              " 'try': 233,\n",
              " 'give': 234,\n",
              " 'husband': 235,\n",
              " 'definitely': 236,\n",
              " 'both': 237,\n",
              " 'where': 238,\n",
              " 'comfy': 239,\n",
              " 'highly': 240,\n",
              " 'store': 241,\n",
              " 'toes': 242,\n",
              " 'wanted': 243,\n",
              " 'arch': 244,\n",
              " 'top': 245,\n",
              " 'getting': 246,\n",
              " 'being': 247,\n",
              " 'light': 248,\n",
              " 'far': 249,\n",
              " 'owned': 250,\n",
              " 'return': 251,\n",
              " 'should': 252,\n",
              " 'something': 253,\n",
              " 'flip': 254,\n",
              " 'received': 255,\n",
              " 'brown': 256,\n",
              " 'thought': 257,\n",
              " 'money': 258,\n",
              " 'seem': 259,\n",
              " 'reviews': 260,\n",
              " 'colors': 261,\n",
              " 'actually': 262,\n",
              " 'absolutely': 263,\n",
              " 'quite': 264,\n",
              " 'durable': 265,\n",
              " 'loved': 266,\n",
              " 'came': 267,\n",
              " 'usually': 268,\n",
              " 'bag': 269,\n",
              " 'extremely': 270,\n",
              " 'tight': 271,\n",
              " 'said': 272,\n",
              " 'see': 273,\n",
              " 'thing': 274,\n",
              " 'does': 275,\n",
              " 'break': 276,\n",
              " 'perfectly': 277,\n",
              " 'those': 278,\n",
              " 'second': 279,\n",
              " 'come': 280,\n",
              " 'makes': 281,\n",
              " 'inside': 282,\n",
              " 'wears': 283,\n",
              " 'box': 284,\n",
              " 'daughter': 285,\n",
              " 'slip': 286,\n",
              " 'couple': 287,\n",
              " 'summer': 288,\n",
              " 'hours': 289,\n",
              " 'needed': 290,\n",
              " 'larger': 291,\n",
              " 'may': 292,\n",
              " 'week': 293,\n",
              " 'son': 294,\n",
              " 'winter': 295,\n",
              " 'shipping': 296,\n",
              " 'probably': 297,\n",
              " 'went': 298,\n",
              " 'dress': 299,\n",
              " 'crocs': 300,\n",
              " 'fits': 301,\n",
              " 'jeans': 302,\n",
              " 'once': 303,\n",
              " 'doesnt': 304,\n",
              " 'sandal': 305,\n",
              " 'stylish': 306,\n",
              " 'christmas': 307,\n",
              " 'water': 308,\n",
              " 'own': 309,\n",
              " 'smaller': 310,\n",
              " 'yet': 311,\n",
              " 'here': 312,\n",
              " 'times': 313,\n",
              " 'flops': 314,\n",
              " 'width': 315,\n",
              " 'casual': 316,\n",
              " 'looked': 317,\n",
              " 'pleased': 318,\n",
              " 'felt': 319,\n",
              " 'less': 320,\n",
              " 'super': 321,\n",
              " 'fine': 322,\n",
              " 'arrived': 323,\n",
              " 'house': 324,\n",
              " 'problems': 325,\n",
              " 'white': 326,\n",
              " 'decided': 327,\n",
              " 'able': 328,\n",
              " 'anything': 329,\n",
              " 'rubber': 330,\n",
              " 'having': 331,\n",
              " 'wish': 332,\n",
              " 'anyone': 333,\n",
              " 'id': 334,\n",
              " 'wet': 335,\n",
              " 'bad': 336,\n",
              " 'people': 337,\n",
              " 'dry': 338,\n",
              " 'sneakers': 339,\n",
              " 'least': 340,\n",
              " 'heels': 341,\n",
              " 'sizes': 342,\n",
              " 'compliments': 343,\n",
              " 'took': 344,\n",
              " 'strap': 345,\n",
              " 'everything': 346,\n",
              " 'snow': 347,\n",
              " 'bottom': 348,\n",
              " 'gift': 349,\n",
              " 'online': 350,\n",
              " 'ankle': 351,\n",
              " 'couldnt': 352,\n",
              " 'finally': 353,\n",
              " 'pain': 354,\n",
              " 'fast': 355,\n",
              " 'hurt': 356,\n",
              " 'seems': 357,\n",
              " 'hiking': 358,\n",
              " 'weeks': 359,\n",
              " 'side': 360,\n",
              " 'outside': 361,\n",
              " 'especially': 362,\n",
              " 'longer': 363,\n",
              " 'quickly': 364,\n",
              " 'three': 365,\n",
              " 'normally': 366,\n",
              " 'sizing': 367,\n",
              " 'part': 368,\n",
              " 'next': 369,\n",
              " 'disappointed': 370,\n",
              " 'easily': 371,\n",
              " 'might': 372,\n",
              " 'heavy': 373,\n",
              " 'large': 374,\n",
              " 'ill': 375,\n",
              " 'thats': 376,\n",
              " 'although': 377,\n",
              " 'wont': 378,\n",
              " 'already': 379,\n",
              " 'regular': 380,\n",
              " 'until': 381,\n",
              " 'started': 382,\n",
              " 'mine': 383,\n",
              " 'exactly': 384,\n",
              " 'cheap': 385,\n",
              " 'material': 386,\n",
              " 's': 387,\n",
              " 'clarks': 388,\n",
              " 'away': 389,\n",
              " 'him': 390,\n",
              " 'hold': 391,\n",
              " 'such': 392,\n",
              " 'extra': 393,\n",
              " 'ordering': 394,\n",
              " 'says': 395,\n",
              " 'end': 396,\n",
              " 'laces': 397,\n",
              " 'bigger': 398,\n",
              " 'uncomfortable': 399,\n",
              " 'live': 400,\n",
              " 'expected': 401,\n",
              " 'using': 402,\n",
              " 'gave': 403,\n",
              " 'normal': 404,\n",
              " 'liked': 405,\n",
              " 'expensive': 406,\n",
              " 'overall': 407,\n",
              " 'straps': 408,\n",
              " 'sturdy': 409,\n",
              " 'everyday': 410,\n",
              " 'past': 411,\n",
              " 'fact': 412,\n",
              " 'stars': 413,\n",
              " 'deal': 414,\n",
              " 'favorite': 415,\n",
              " 'month': 416,\n",
              " 'things': 417,\n",
              " 'miles': 418,\n",
              " 'why': 419,\n",
              " 'myself': 420,\n",
              " 'glad': 421,\n",
              " 'real': 422,\n",
              " 'service': 423,\n",
              " 'replace': 424,\n",
              " 'slipper': 425,\n",
              " 'short': 426,\n",
              " 'maybe': 427,\n",
              " 'wrong': 428,\n",
              " 'must': 429,\n",
              " 'design': 430,\n",
              " 'youre': 431,\n",
              " 'left': 432,\n",
              " 'awesome': 433,\n",
              " 'havent': 434,\n",
              " 'mens': 435,\n",
              " 'kind': 436,\n",
              " 'cold': 437,\n",
              " 'trying': 438,\n",
              " 'wonderful': 439,\n",
              " 'between': 440,\n",
              " 'nothing': 441,\n",
              " 'review': 442,\n",
              " 'thick': 443,\n",
              " 'beautiful': 444,\n",
              " 'feels': 445,\n",
              " 'room': 446,\n",
              " 'full': 447,\n",
              " 'weight': 448,\n",
              " 'us': 449,\n",
              " 'else': 450,\n",
              " 'stretch': 451,\n",
              " 'plus': 452,\n",
              " 'front': 453,\n",
              " 'believe': 454,\n",
              " 'apart': 455,\n",
              " 'trip': 456,\n",
              " 'snug': 457,\n",
              " 'clean': 458,\n",
              " 'hope': 459,\n",
              " 'pay': 460,\n",
              " 'during': 461,\n",
              " 'type': 462,\n",
              " 'recommended': 463,\n",
              " 'flat': 464,\n",
              " 'area': 465,\n",
              " 'stay': 466,\n",
              " 'home': 467,\n",
              " 'our': 468,\n",
              " 'others': 469,\n",
              " 'due': 470,\n",
              " 'wouldnt': 471,\n",
              " 'item': 472,\n",
              " 'classic': 473,\n",
              " 'each': 474,\n",
              " 'sale': 475,\n",
              " 'slightly': 476,\n",
              " 'making': 477,\n",
              " 'under': 478,\n",
              " 'available': 479,\n",
              " 'cost': 480,\n",
              " 'thin': 481,\n",
              " 'weather': 482,\n",
              " 'tell': 483,\n",
              " 'instead': 484,\n",
              " 'model': 485,\n",
              " 'thanks': 486,\n",
              " 'picture': 487,\n",
              " 'company': 488,\n",
              " 'returned': 489,\n",
              " 'lasted': 490,\n",
              " 'traction': 491,\n",
              " 'wife': 492,\n",
              " 'place': 493,\n",
              " 'red': 494,\n",
              " 'daily': 495,\n",
              " 'hot': 496,\n",
              " 'stiff': 497,\n",
              " 'suede': 498,\n",
              " 'uggs': 499,\n",
              " 'womens': 500,\n",
              " 'help': 501,\n",
              " 'fall': 502,\n",
              " 'works': 503,\n",
              " 'waterproof': 504,\n",
              " 'paid': 505,\n",
              " 'broken': 506,\n",
              " 'reason': 507,\n",
              " 'arent': 508,\n",
              " 'either': 509,\n",
              " 'let': 510,\n",
              " 'stores': 511,\n",
              " 'wasnt': 512,\n",
              " 'job': 513,\n",
              " 'saw': 514,\n",
              " 'shape': 515,\n",
              " 'etc': 516,\n",
              " 'blisters': 517,\n",
              " 'goes': 518,\n",
              " 'soon': 519,\n",
              " 'cannot': 520,\n",
              " 'loose': 521,\n",
              " 'm': 522,\n",
              " 'read': 523,\n",
              " 'rather': 524,\n",
              " 'finding': 525,\n",
              " 'tennis': 526,\n",
              " 'expect': 527,\n",
              " 'amazing': 528,\n",
              " 'broke': 529,\n",
              " 'today': 530,\n",
              " 'cool': 531,\n",
              " 'line': 532,\n",
              " 'completely': 533,\n",
              " 'guess': 534,\n",
              " 'issue': 535,\n",
              " 'everyone': 536,\n",
              " 'insole': 537,\n",
              " 'often': 538,\n",
              " 'value': 539,\n",
              " 'isnt': 540,\n",
              " 'similar': 541,\n",
              " 'original': 542,\n",
              " 'lots': 543,\n",
              " 'school': 544,\n",
              " 'difficult': 545,\n",
              " 'gotten': 546,\n",
              " 'held': 547,\n",
              " 'low': 548,\n",
              " 'brands': 549,\n",
              " 'friend': 550,\n",
              " 'ice': 551,\n",
              " 'wider': 552,\n",
              " 'someone': 553,\n",
              " 'thank': 554,\n",
              " 'sent': 555,\n",
              " 'show': 556,\n",
              " 'within': 557,\n",
              " 'satisfied': 558,\n",
              " 'plastic': 559,\n",
              " 'cushion': 560,\n",
              " 'working': 561,\n",
              " 'seller': 562,\n",
              " 'local': 563,\n",
              " 'breaking': 564,\n",
              " 'insoles': 565,\n",
              " 'kept': 566,\n",
              " 'quick': 567,\n",
              " 'comes': 568,\n",
              " 'unfortunately': 569,\n",
              " 'delivery': 570,\n",
              " 'adidas': 571,\n",
              " 'stand': 572,\n",
              " 'person': 573,\n",
              " 'previous': 574,\n",
              " 'lightweight': 575,\n",
              " 'fan': 576,\n",
              " 'provide': 577,\n",
              " 'noticed': 578,\n",
              " 'life': 579,\n",
              " 'otherwise': 580,\n",
              " 'experience': 581,\n",
              " 'lining': 582,\n",
              " 'blue': 583,\n",
              " 'likes': 584,\n",
              " 'height': 585,\n",
              " 'cushioning': 586,\n",
              " 'styles': 587,\n",
              " 'cheaper': 588,\n",
              " 'start': 589,\n",
              " 'period': 590,\n",
              " 'needs': 591,\n",
              " 'recently': 592,\n",
              " 'across': 593,\n",
              " 'shopping': 594,\n",
              " 'pants': 595,\n",
              " 'legs': 596,\n",
              " 'upper': 597,\n",
              " 'coming': 598,\n",
              " 'medium': 599,\n",
              " 'sneaker': 600,\n",
              " 'free': 601,\n",
              " 'ended': 602,\n",
              " 'purchasing': 603,\n",
              " 'worked': 604,\n",
              " 'customer': 605,\n",
              " 'later': 606,\n",
              " 'runs': 607,\n",
              " 'balance': 608,\n",
              " 'cut': 609,\n",
              " 'rockport': 610,\n",
              " 'everywhere': 611,\n",
              " 'five': 612,\n",
              " 'case': 613,\n",
              " 'care': 614,\n",
              " 'walked': 615,\n",
              " 'rain': 616,\n",
              " 'wallet': 617,\n",
              " 'gets': 618,\n",
              " 'change': 619,\n",
              " 'whole': 620,\n",
              " 'length': 621,\n",
              " 'course': 622,\n",
              " 'products': 623,\n",
              " 'seen': 624,\n",
              " 'nearly': 625,\n",
              " 'thinking': 626,\n",
              " 'wait': 627,\n",
              " 'construction': 628,\n",
              " 'given': 629,\n",
              " 'usual': 630,\n",
              " 'plan': 631,\n",
              " 'told': 632,\n",
              " 'solid': 633,\n",
              " 'velcro': 634,\n",
              " 'tend': 635,\n",
              " 'office': 636,\n",
              " 'ok': 637,\n",
              " 'complaint': 638,\n",
              " 'sometimes': 639,\n",
              " 'business': 640,\n",
              " 'padding': 641,\n",
              " 'ecco': 642,\n",
              " 'difference': 643,\n",
              " 'china': 644,\n",
              " 'fantastic': 645,\n",
              " 'knew': 646,\n",
              " 'doing': 647,\n",
              " 'beat': 648,\n",
              " 'youll': 649,\n",
              " 'beach': 650,\n",
              " 'point': 651,\n",
              " 'footwear': 652,\n",
              " 'replacement': 653,\n",
              " 'reading': 654,\n",
              " 'nicely': 655,\n",
              " 'four': 656,\n",
              " 'surprised': 657,\n",
              " 'hoping': 658,\n",
              " 'skechers': 659,\n",
              " 'spend': 660,\n",
              " 'fell': 661,\n",
              " 'send': 662,\n",
              " 'seemed': 663,\n",
              " 'carry': 664,\n",
              " 'standing': 665,\n",
              " 'except': 666,\n",
              " 'friends': 667,\n",
              " 'kids': 668,\n",
              " 'exact': 669,\n",
              " 'ankles': 670,\n",
              " 'issues': 671,\n",
              " 'anywhere': 672,\n",
              " 'grip': 673,\n",
              " 'air': 674,\n",
              " 'night': 675,\n",
              " 'particular': 676,\n",
              " 'third': 677,\n",
              " 'version': 678,\n",
              " 'step': 679,\n",
              " 'durability': 680,\n",
              " 'trouble': 681,\n",
              " 'close': 682,\n",
              " 'gone': 683,\n",
              " 'w': 684,\n",
              " 'dark': 685,\n",
              " 'pink': 686,\n",
              " 'clogs': 687,\n",
              " 'shipped': 688,\n",
              " 'fairly': 689,\n",
              " 'immediately': 690,\n",
              " 'slide': 691,\n",
              " 'birkenstock': 692,\n",
              " 'add': 693,\n",
              " 'slippery': 694,\n",
              " 'become': 695,\n",
              " 'condition': 696,\n",
              " 'stitching': 697,\n",
              " 'keeps': 698,\n",
              " 'plenty': 699,\n",
              " 'unless': 700,\n",
              " 'enjoy': 701,\n",
              " 'choice': 702,\n",
              " 'shop': 703,\n",
              " 'totally': 704,\n",
              " 'tread': 705,\n",
              " 'anymore': 706,\n",
              " 'fabric': 707,\n",
              " 'based': 708,\n",
              " 'higher': 709,\n",
              " 'suggest': 710,\n",
              " 'hour': 711,\n",
              " 'sexy': 712,\n",
              " 'smell': 713,\n",
              " 'name': 714,\n",
              " 'flexible': 715,\n",
              " 'simply': 716,\n",
              " 'compared': 717,\n",
              " 'sides': 718,\n",
              " 'continue': 719,\n",
              " 'asics': 720,\n",
              " 'huge': 721,\n",
              " 'green': 722,\n",
              " 'stuff': 723,\n",
              " 'dressy': 724,\n",
              " 'dirty': 725,\n",
              " 'ran': 726,\n",
              " 'reasonable': 727,\n",
              " 'mind': 728,\n",
              " 'materials': 729,\n",
              " 'anyway': 730,\n",
              " 'done': 731,\n",
              " 'correct': 732,\n",
              " 'tough': 733,\n",
              " 'forever': 734,\n",
              " 'feeling': 735,\n",
              " 'easier': 736,\n",
              " 'fun': 737,\n",
              " 'tongue': 738,\n",
              " 'flop': 739,\n",
              " 'strong': 740,\n",
              " 'amount': 741,\n",
              " 'exchange': 742,\n",
              " 'inserts': 743,\n",
              " 'gives': 744,\n",
              " 'twice': 745,\n",
              " 'simple': 746,\n",
              " 'ugg': 747,\n",
              " 'attractive': 748,\n",
              " 'purse': 749,\n",
              " 'breakin': 750,\n",
              " 'supportive': 751,\n",
              " 'keen': 752,\n",
              " 'falling': 753,\n",
              " 'spring': 754,\n",
              " 'lace': 755,\n",
              " 'dance': 756,\n",
              " 'fitting': 757,\n",
              " 'alot': 758,\n",
              " 'sore': 759,\n",
              " 'forward': 760,\n",
              " 'floors': 761,\n",
              " 'birthday': 762,\n",
              " 'open': 763,\n",
              " 'chance': 764,\n",
              " 'hes': 765,\n",
              " 'future': 766,\n",
              " 'walks': 767,\n",
              " 'boyfriend': 768,\n",
              " 'star': 769,\n",
              " 'floor': 770,\n",
              " 'flipflops': 771,\n",
              " 'impressed': 772,\n",
              " 'th': 773,\n",
              " 'play': 774,\n",
              " 'holding': 775,\n",
              " 'polish': 776,\n",
              " 'starting': 777,\n",
              " 'knee': 778,\n",
              " 'putting': 779,\n",
              " 'reviewers': 780,\n",
              " 'returning': 781,\n",
              " 'yes': 782,\n",
              " 'matter': 783,\n",
              " 'inch': 784,\n",
              " 'offer': 785,\n",
              " 'remember': 786,\n",
              " 'steel': 787,\n",
              " 'zipper': 788,\n",
              " 'sized': 789,\n",
              " 'along': 790,\n",
              " 'prefer': 791,\n",
              " 'mom': 792,\n",
              " 'match': 793,\n",
              " 'please': 794,\n",
              " 'footbed': 795,\n",
              " 'wedding': 796,\n",
              " 'arches': 797,\n",
              " 'comfortably': 798,\n",
              " 'theres': 799,\n",
              " 'cause': 800,\n",
              " 'converse': 801,\n",
              " 'built': 802,\n",
              " 'against': 803,\n",
              " 'heard': 804,\n",
              " 'minutes': 805,\n",
              " 'spent': 806,\n",
              " 'takes': 807,\n",
              " 'helps': 808,\n",
              " 'hike': 809,\n",
              " 'important': 810,\n",
              " 'wash': 811,\n",
              " 'amazoncom': 812,\n",
              " 'together': 813,\n",
              " 'average': 814,\n",
              " 'decent': 815,\n",
              " 'nike': 816,\n",
              " 'opinion': 817,\n",
              " 'tall': 818,\n",
              " 'insert': 819,\n",
              " 'sold': 820,\n",
              " 'em': 821,\n",
              " 'man': 822,\n",
              " 'pull': 823,\n",
              " 'number': 824,\n",
              " 'considering': 825,\n",
              " 'replaced': 826,\n",
              " 'older': 827,\n",
              " 'keeping': 828,\n",
              " 'adorable': 829,\n",
              " 'note': 830,\n",
              " 'certainly': 831,\n",
              " 'women': 832,\n",
              " 'entire': 833,\n",
              " 'lower': 834,\n",
              " 'website': 835,\n",
              " 'moccasins': 836,\n",
              " 'reef': 837,\n",
              " 'street': 838,\n",
              " 'idea': 839,\n",
              " 'taking': 840,\n",
              " 'season': 841,\n",
              " 'stop': 842,\n",
              " 'ball': 843,\n",
              " 'e': 844,\n",
              " 'dog': 845,\n",
              " 'pocket': 846,\n",
              " 'holes': 847,\n",
              " 'agree': 848,\n",
              " 'giving': 849,\n",
              " 'rocks': 850,\n",
              " 'check': 851,\n",
              " 'site': 852,\n",
              " 'added': 853,\n",
              " 'hate': 854,\n",
              " 'rest': 855,\n",
              " 'present': 856,\n",
              " 'priced': 857,\n",
              " 'holds': 858,\n",
              " 'notice': 859,\n",
              " 'set': 860,\n",
              " 'dirt': 861,\n",
              " 'slipping': 862,\n",
              " 'tired': 863,\n",
              " 'mud': 864,\n",
              " 'heal': 865,\n",
              " 'birkenstocks': 866,\n",
              " 'merrell': 867,\n",
              " 'leave': 868,\n",
              " 'spirit': 869,\n",
              " 'glove': 870,\n",
              " 'fashion': 871,\n",
              " 'dr': 872,\n",
              " 'surfaces': 873,\n",
              " 'world': 874,\n",
              " 'means': 875,\n",
              " 'poor': 876,\n",
              " 'calves': 877,\n",
              " 'skin': 878,\n",
              " 'allow': 879,\n",
              " 'navy': 880,\n",
              " 'description': 881,\n",
              " 'near': 882,\n",
              " 'wants': 883,\n",
              " 'including': 884,\n",
              " 'called': 885,\n",
              " 'consider': 886,\n",
              " 'ask': 887,\n",
              " 'smooth': 888,\n",
              " 'ugly': 889,\n",
              " 'leg': 890,\n",
              " 'incredibly': 891,\n",
              " 'lost': 892,\n",
              " 'rub': 893,\n",
              " 'class': 894,\n",
              " 'tan': 895,\n",
              " 'uppers': 896,\n",
              " 'werent': 897,\n",
              " 'careful': 898,\n",
              " 'tie': 899,\n",
              " 'mail': 900,\n",
              " 'six': 901,\n",
              " 'trail': 902,\n",
              " 'theyve': 903,\n",
              " 'throw': 904,\n",
              " 'desert': 905,\n",
              " 'plantar': 906,\n",
              " 'appear': 907,\n",
              " 'reebok': 908,\n",
              " 'above': 909,\n",
              " 'painful': 910,\n",
              " 'boat': 911,\n",
              " 'mostly': 912,\n",
              " 'nd': 913,\n",
              " 'inches': 914,\n",
              " 'piece': 915,\n",
              " 'truly': 916,\n",
              " 'delivered': 917,\n",
              " 'oh': 918,\n",
              " 'happened': 919,\n",
              " 'skirts': 920,\n",
              " 'guy': 921,\n",
              " 'foam': 922,\n",
              " 'penny': 923,\n",
              " 'lighter': 924,\n",
              " 'roomy': 925,\n",
              " 'turned': 926,\n",
              " 'complaints': 927,\n",
              " 'mile': 928,\n",
              " 'knees': 929,\n",
              " 'bed': 930,\n",
              " 'gel': 931,\n",
              " 'despite': 932,\n",
              " 'metal': 933,\n",
              " 'reviewer': 934,\n",
              " 'theyll': 935,\n",
              " 'rough': 936,\n",
              " 'per': 937,\n",
              " 'elastic': 938,\n",
              " 'dollars': 939,\n",
              " 'soccer': 940,\n",
              " 'thong': 941,\n",
              " 'asked': 942,\n",
              " 'cushioned': 943,\n",
              " 'bags': 944,\n",
              " 'classy': 945,\n",
              " 'd': 946,\n",
              " 'constructed': 947,\n",
              " 'hole': 948,\n",
              " 'mother': 949,\n",
              " 'closet': 950,\n",
              " 'changed': 951,\n",
              " 'form': 952,\n",
              " 'ready': 953,\n",
              " 'mention': 954,\n",
              " 'rock': 955,\n",
              " 'family': 956,\n",
              " 'concrete': 957,\n",
              " 'morning': 958,\n",
              " 'standard': 959,\n",
              " 'sand': 960,\n",
              " 'stick': 961,\n",
              " 'unlike': 962,\n",
              " 'town': 963,\n",
              " 'patent': 964,\n",
              " 'eventually': 965,\n",
              " 'car': 966,\n",
              " 'versatile': 967,\n",
              " 'thrilled': 968,\n",
              " 'travel': 969,\n",
              " 'dad': 970,\n",
              " 'vacation': 971,\n",
              " 'cleats': 972,\n",
              " 'somewhat': 973,\n",
              " 'designed': 974,\n",
              " 'literally': 975,\n",
              " 'girl': 976,\n",
              " 'bottoms': 977,\n",
              " 'refund': 978,\n",
              " 'american': 979,\n",
              " 'rating': 980,\n",
              " 'double': 981,\n",
              " 'discomfort': 982,\n",
              " 'terrain': 983,\n",
              " 'instep': 984,\n",
              " 'orthotics': 985,\n",
              " 'gold': 986,\n",
              " 'excited': 987,\n",
              " 'taken': 988,\n",
              " 'known': 989,\n",
              " 'tear': 990,\n",
              " 'professional': 991,\n",
              " 'padded': 992,\n",
              " 'pockets': 993,\n",
              " 'mentioned': 994,\n",
              " 'outdoor': 995,\n",
              " 'woman': 996,\n",
              " 'perhaps': 997,\n",
              " 'straight': 998,\n",
              " 'calf': 999,\n",
              " 'slacks': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "vocab_to_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51e6QM5Bx66i",
        "outputId": "7413386c-2bf8-4242-e14d-c772af2900b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3721, 71, 199, 8, 402, 22, 29, 1, 14, 327, 5, 3057, 10, 980, 5, 413, 1, 100, 40, 2, 15, 6, 48, 33, 3, 673, 6878, 210, 22, 673, 1, 454, 16, 1263, 11, 4, 2498, 111, 7, 680, 2, 330, 182, 321, 2848, 16, 222, 3, 10836, 524, 364, 1, 3856, 21, 4, 3857, 41, 2849, 67, 8944, 15, 10, 32, 535, 3, 119, 272, 18, 8944, 1273, 14995, 330, 5, 1333, 41, 834, 1016, 119, 272, 18, 112, 337, 183, 70, 8944, 14, 2, 4338, 8, 4153, 21, 144, 330, 303, 6, 13, 506, 11, 1, 47, 100, 108, 12, 15, 5, 333, 183, 16, 777, 55, 20, 181, 11, 728, 6, 52, 32, 205, 364, 396, 3721, 106, 82, 1232, 1233, 7, 67, 148, 199, 3, 71, 246, 14996, 21, 10, 8945, 221, 4748, 15, 1, 327, 1, 86, 290, 4, 23, 8, 10, 309, 5, 2644, 5014, 1, 38, 82, 4749, 17, 1, 40, 12, 7, 2, 1777, 1584, 163, 1, 224, 17, 4, 1811, 8, 15, 11, 573, 5, 168, 231, 1, 646, 105, 1, 30, 246, 19, 30, 4, 2645, 65, 440, 12, 3, 14997, 5766, 14998, 435, 1232, 29, 237, 1263, 4, 33, 48, 210, 1, 319, 18, 2, 4339, 955, 14999, 38, 320, 8, 4, 2687, 184, 107, 2, 247, 272, 12, 14, 4, 33, 184, 284, 3, 1353, 8946, 167, 5, 577, 699, 8, 163, 50, 5380, 17, 1123, 858, 12, 89, 673, 10, 341, 270, 56, 24, 142, 2283, 3, 2, 40, 508, 4, 207, 1, 1105, 9, 461, 115, 5767, 20, 6, 13, 27, 167, 5, 868, 17, 7, 1458, 1098, 8, 10837, 838, 15, 1, 32, 88, 3, 1, 40, 10, 1232, 15, 271, 25, 3593, 271, 20, 5768, 1, 30, 85, 4, 11, 2, 4748, 6268, 8947, 41, 10, 1088, 210, 1, 30, 657, 18, 1, 38, 5, 35, 205, 5, 4, 11, 12, 5, 57, 2, 107, 1, 30, 123, 7, 136, 12, 508, 1491, 896, 1, 1492, 527, 9, 5, 451, 4, 132, 20, 28, 8, 90, 6, 13, 127, 3, 271, 2, 5015, 86, 577, 4, 134, 1186, 8, 48, 3, 10838, 1, 1334, 18, 2, 155, 185, 5, 114, 4, 1232, 29, 16, 5, 79, 233, 19, 17, 11, 573, 3594, 4, 5381, 21, 18, 29, 3, 175, 7, 121, 766, 3490, 160, 19, 15000, 22, 16, 238, 331, 75, 309, 15, 86, 5769, 111, 172, 1, 73, 12, 1, 30, 647, 3, 103, 5014, 71, 246, 12, 1, 15001, 103, 8, 2, 5014, 18, 1, 3722, 17, 172, 3, 1565, 726, 65, 9, 99, 90, 8948, 1403, 5014, 3, 799, 4, 99, 987, 5, 97, 12, 15, 14, 4, 1134, 15002, 101, 1061, 33, 3723, 7, 2391, 8949, 5014, 3, 167, 1286, 5, 572, 17, 1123, 858, 4154, 2, 184, 16, 481, 167, 5, 5382, 217, 3058, 2357, 3, 2, 142, 91, 4, 5770, 673, 101, 86, 7735, 858, 10, 15003, 91, 3724, 3725, 914, 21, 12, 3, 1, 63, 6269, 65, 694, 8950, 220, 371, 90, 2, 284, 91, 103, 1251, 67, 15004, 330, 17, 2, 211, 3, 1, 63, 2987, 5, 96, 15005, 24, 249, 1, 434, 38, 5, 81, 329, 5, 12, 5, 1523, 9, 58, 3133, 28, 21, 121, 29, 31, 219, 5, 510, 9, 674, 55, 71, 474, 2850, 389, 58, 2988, 1984, 248, 2989, 1, 14, 82, 1232, 67, 212, 4, 293, 11, 12, 7, 230, 359, 90, 3, 6, 77, 556, 4, 2317, 8, 32, 1, 1055, 303, 1, 3595, 1141, 1, 1887, 6879, 7736, 1232, 91, 19, 44, 33, 15006, 15007, 15008, 3, 686, 7737, 4, 132, 1096, 6, 372, 589, 5, 32, 20, 381, 175, 6, 259, 56, 15009, 1, 47, 200, 22, 16, 4, 33, 29, 7, 1233, 1232, 331, 25, 10839, 1241, 121, 422, 955, 311, 1, 189, 2392, 2182, 96, 1439, 210, 1, 47, 1055, 18, 19, 47, 1929, 42, 322, 17, 422, 955, 28, 19, 275, 17, 1252, 278, 1233, 15010, 13, 1812, 55, 8], [12, 52, 234, 31, 354, 1, 34, 12, 15, 68, 6, 317, 524, 216, 11, 2, 487, 3, 825, 6, 897, 42, 4, 23, 8, 15, 1, 257, 6, 47, 36, 973, 27, 53, 25, 7738, 5, 32, 1888, 1075, 12, 15, 52, 609, 75, 341, 6, 177, 4, 78, 173, 24, 1, 710, 246, 41, 340, 4, 227, 35, 291, 53, 31, 219, 5, 81, 329, 1529, 1440, 3, 123, 220, 81, 25, 70, 12, 15, 1, 615, 7, 320, 60, 805, 172, 6, 382, 3394, 3, 4340, 10, 341, 6, 89, 13, 4, 78, 271, 17, 2, 242, 24, 53, 31, 14, 4, 134, 87, 12, 236, 13, 25, 7, 31, 1, 14, 5, 1216, 6, 13, 3858, 3, 216, 20, 6, 13, 25, 228, 2, 354], [43, 22, 2183, 261, 583, 8951, 15011, 13, 42, 24, 444]]\n"
          ]
        }
      ],
      "source": [
        "#encoding of reviews (replace words in our reviews by integers)\n",
        "reviews_int = []\n",
        "for review in text:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    reviews_int.append(r)\n",
        "print (reviews_int[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tWUS-v_lyZ3B"
      },
      "outputs": [],
      "source": [
        "labels = np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9Y2AsEgiHd3D"
      },
      "outputs": [],
      "source": [
        "#converting to binary class\n",
        "l=[]\n",
        "for i in labels:\n",
        "  if(i==0 or i==1 or i==2):\n",
        "    i=0\n",
        "  elif(i==3 or i==4 or i==5):\n",
        "    i=1\n",
        "  l.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XjU1VYQgJzqu"
      },
      "outputs": [],
      "source": [
        "label = np.array(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1bSxpAvy11B",
        "outputId": "7deec77c-6373-43aa-9db2-067b1a97d57d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.,  0.,  1., ...,  1.,  1., nan])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NKUwmJB9zvV9"
      },
      "outputs": [],
      "source": [
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iynsujgC0C6G"
      },
      "outputs": [],
      "source": [
        "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
        "\n",
        "reviews = pad_input(reviews_int, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLI5_yvY0eoY",
        "outputId": "3d54c4d0-75fe-4678-f96f-c839e8fd0c97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3721,    71,   199,     8,   402,    22,    29,     1,    14,\n",
              "         327,     5,  3057,    10,   980,     5,   413,     1,   100,\n",
              "          40,     2,    15,     6,    48,    33,     3,   673,  6878,\n",
              "         210,    22,   673,     1,   454,    16,  1263,    11,     4,\n",
              "        2498,   111,     7,   680,     2,   330,   182,   321,  2848,\n",
              "          16,   222,     3, 10836,   524,   364,     1,  3856,    21,\n",
              "           4,  3857,    41,  2849,    67,  8944,    15,    10,    32,\n",
              "         535,     3,   119,   272,    18,  8944,  1273, 14995,   330,\n",
              "           5,  1333,    41,   834,  1016,   119,   272,    18,   112,\n",
              "         337,   183,    70,  8944,    14,     2,  4338,     8,  4153,\n",
              "          21,   144,   330,   303,     6,    13,   506,    11,     1,\n",
              "          47,   100,   108,    12,    15,     5,   333,   183,    16,\n",
              "         777,    55,    20,   181,    11,   728,     6,    52,    32,\n",
              "         205,   364,   396,  3721,   106,    82,  1232,  1233,     7,\n",
              "          67,   148,   199,     3,    71,   246, 14996,    21,    10,\n",
              "        8945,   221,  4748,    15,     1,   327,     1,    86,   290,\n",
              "           4,    23,     8,    10,   309,     5,  2644,  5014,     1,\n",
              "          38,    82,  4749,    17,     1,    40,    12,     7,     2,\n",
              "        1777,  1584,   163,     1,   224,    17,     4,  1811,     8,\n",
              "          15,    11,   573,     5,   168,   231,     1,   646,   105,\n",
              "           1,    30,   246,    19,    30,     4,  2645,    65,   440,\n",
              "          12,     3, 14997,  5766, 14998,   435,  1232,    29,   237,\n",
              "        1263,     4])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "reviews[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GK10eByN0iTZ"
      },
      "outputs": [],
      "source": [
        "#80% train, 10% test & 10% validation\n",
        "split_frac = 0.8\n",
        "len_feat = len(reviews)\n",
        "train_x = reviews[0:int(split_frac*len_feat)]\n",
        "train_y = label[0:int(split_frac*len_feat)]\n",
        "remaining_x = reviews[int(split_frac*len_feat):]\n",
        "remaining_y = label[int(split_frac*len_feat):]\n",
        "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
        "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
        "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
        "test_y = remaining_y[int(len(remaining_y)*0.5):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cWDjPD6C1TM1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jlFbIryf3z-k"
      },
      "outputs": [],
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9beZSKf42k3",
        "outputId": "4f9ac7ff-cf09-4f2b-d912-18c5af50aa69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input size:  torch.Size([64, 200])\n",
            "Sample input: \n",
            " tensor([[   0,    0,    0,  ...,   76,   30, 2184],\n",
            "        [   0,    0,    0,  ...,  149,    5,  333],\n",
            "        [   0,    0,    0,  ...,  404,    3,   59],\n",
            "        ...,\n",
            "        [   0,    0,    0,  ...,    1,   43,    9],\n",
            "        [   0,    0,    0,  ...,   23,    8,  341],\n",
            "        [   0,    0,    0,  ...,  103, 3117,  585]])\n",
            "\n",
            "Sample label size:  torch.Size([64])\n",
            "Sample label: \n",
            " tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zHIIVDg4YCH"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NEUiimnclYgl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,dropout=drop_prob, batch_first=True)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3).to(device)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size).to(device)\n",
        "        self.sig = nn.Sigmoid().to(device)\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).normal_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).normal_().to(device))\n",
        "        \n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_IdgQuFRdKB"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ljFijODkRojC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers,dropout=drop_prob, batch_first=True)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3).to(device)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size).to(device)\n",
        "        self.sig = nn.Sigmoid().to(device)\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        gru_out, hidden = self.gru(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(gru_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)        \n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6iBirSkRpDJ"
      },
      "source": [
        "#Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "N-VZcluhLBLp"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400 #\n",
        "batch_size = 64\n",
        "device = torch.device(\"cuda\")\n",
        "hidden_dim = 256\n",
        "n_layers = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LYT_aAYRgg4"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-Em4lCwSLHAK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# training params\n",
        "def train(train_loader,valid_loader,epochs,model_type):\n",
        "  if model_type == \"LSTM\":\n",
        "    net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).to(device)\n",
        "  elif (model_type == \"GRU\"):\n",
        "    net = GRU(vocab_size, output_size, embedding_dim, hidden_dim, n_layers).to(device)\n",
        "  lr=0.001\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "  counter = 0\n",
        "  print_every = 100\n",
        "  clip=5 # gradient clipping\n",
        "  net.train()\n",
        "  # train for some number of epochs\n",
        "  for e in range(epochs):\n",
        "      # initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "\n",
        "      # batch loop\n",
        "      for inputs, labels in train_loader:\n",
        "          counter += 1\n",
        "\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          # Creating new variables for the hidden state, otherwise\n",
        "          # we'd backprop through the entire training history\n",
        "          if model_type == \"GRU\":\n",
        "            h = h.data.to(device)\n",
        "          elif model_type == \"LSTM\":\n",
        "            h = tuple([each.data.to(device) for each in h])\n",
        "          # zero accumulated gradients\n",
        "          net.zero_grad()\n",
        "\n",
        "          # get the output from the model\n",
        "          inputs = inputs.type(torch.LongTensor).to(device)\n",
        "          output, h = net(inputs, h)\n",
        "          #print(output[0])\n",
        "          # calculate the loss and perform backprop\n",
        "          #print(\"shape of output\", output.shape)\n",
        "          #print(\"shape of labels\", labels.shape)\n",
        "          loss = criterion(output.squeeze(), labels.float())\n",
        "          loss.backward()\n",
        "          # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "          nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "          optimizer.step()\n",
        "\n",
        "          # loss stats\n",
        "          if counter % print_every == 0:\n",
        "              # Get validation loss\n",
        "              val_h = net.init_hidden(batch_size)\n",
        "              val_losses = []\n",
        "              net.eval()\n",
        "              for inputs, labels in valid_loader:\n",
        "\n",
        "                  # Creating new variables for the hidden state, otherwise\n",
        "                  # we'd backprop through the entire training history\n",
        "                  if model_type == \"GRU\":\n",
        "                    val_h = val_h.data.to(device)\n",
        "                  elif (model_type == \"LSTM\"):\n",
        "                    val_h = tuple([each.data.to(device) for each in val_h])\n",
        "\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                  inputs = inputs.type(torch.LongTensor).to(device)\n",
        "                  output, val_h = net(inputs, val_h)\n",
        "                  val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                  val_losses.append(val_loss.item())\n",
        "\n",
        "              net.train()\n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Model:{}...\".format(model_type),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "  return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4JiM6N3-O0Fa"
      },
      "outputs": [],
      "source": [
        "train_on_gpu = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qv9cWzjUEOj"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "C-ZNqtjkLZQZ"
      },
      "outputs": [],
      "source": [
        "def test(net,test_loader,model_type):\n",
        "\n",
        "  # Get test data loss and accuracy\n",
        "  lr=0.001\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "  test_losses = [] # track loss\n",
        "  num_correct = 0\n",
        "\n",
        "  # init hidden state\n",
        "  h = net.init_hidden(batch_size)\n",
        "\n",
        "  net.eval()\n",
        "  # iterate over test data\n",
        "  for inputs, labels in test_loader:\n",
        "\n",
        "      # Creating new variables for the hidden state, otherwise\n",
        "      # we'd backprop through the entire training history\n",
        "      if model_type == \"GRU\":\n",
        "        h = h.data.to(device)\n",
        "      elif (model_type == \"LSTM\"):\n",
        "        h = tuple([each.data.to(device) for each in h])\n",
        "\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      # get predicted outputs\n",
        "      inputs = inputs.type(torch.LongTensor).to(device)\n",
        "      output, h = net(inputs, h)\n",
        "      \n",
        "      # calculate loss\n",
        "      test_loss = criterion(output.squeeze(), labels.float())\n",
        "      test_losses.append(test_loss.item())\n",
        "      \n",
        "      # convert output probabilities to predicted class (0 or 1)\n",
        "      pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "      \n",
        "      # compare predictions to true label\n",
        "      correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "  # -- stats! -- ##\n",
        "  # avg test loss\n",
        "  print(\"Model: {}\".format(model_type))\n",
        "  print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "  # accuracy over all test data\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZHrB-4L2-lQ"
      },
      "source": [
        "#Evaluating our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w8C-tZTOP4D",
        "outputId": "04fc2bad-5213-4dc9-f73e-72f3079ffd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5... Model:LSTM... Step: 100... Loss: 0.270032... Val Loss: 0.285302\n",
            "Epoch: 1/5... Model:LSTM... Step: 200... Loss: 0.100711... Val Loss: 0.256972\n",
            "Epoch: 1/5... Model:LSTM... Step: 300... Loss: 0.290094... Val Loss: 0.250686\n",
            "Epoch: 2/5... Model:LSTM... Step: 400... Loss: 0.265768... Val Loss: 0.223503\n",
            "Epoch: 2/5... Model:LSTM... Step: 500... Loss: 0.179729... Val Loss: 0.224276\n",
            "Epoch: 2/5... Model:LSTM... Step: 600... Loss: 0.161839... Val Loss: 0.215681\n",
            "Epoch: 3/5... Model:LSTM... Step: 700... Loss: 0.107302... Val Loss: 0.217033\n",
            "Epoch: 3/5... Model:LSTM... Step: 800... Loss: 0.066913... Val Loss: 0.224193\n",
            "Epoch: 3/5... Model:LSTM... Step: 900... Loss: 0.086534... Val Loss: 0.194190\n",
            "Epoch: 4/5... Model:LSTM... Step: 1000... Loss: 0.065697... Val Loss: 0.220132\n",
            "Epoch: 4/5... Model:LSTM... Step: 1100... Loss: 0.091139... Val Loss: 0.256335\n",
            "Epoch: 4/5... Model:LSTM... Step: 1200... Loss: 0.208574... Val Loss: 0.250754\n",
            "Epoch: 4/5... Model:LSTM... Step: 1300... Loss: 0.029040... Val Loss: 0.241461\n",
            "Epoch: 5/5... Model:LSTM... Step: 1400... Loss: 0.013782... Val Loss: 0.312671\n",
            "Epoch: 5/5... Model:LSTM... Step: 1500... Loss: 0.043314... Val Loss: 0.265414\n",
            "Epoch: 5/5... Model:LSTM... Step: 1600... Loss: 0.076166... Val Loss: 0.293723\n",
            "Epoch: 1/1... Model:GRU... Step: 100... Loss: 0.493203... Val Loss: 0.327818\n",
            "Epoch: 1/1... Model:GRU... Step: 200... Loss: 0.169580... Val Loss: 0.226335\n",
            "Epoch: 1/1... Model:GRU... Step: 300... Loss: 0.170994... Val Loss: 0.206895\n"
          ]
        }
      ],
      "source": [
        "lstm = train(train_loader,valid_loader,5,model_type=\"LSTM\")\n",
        "gru = train(train_loader,valid_loader,1,model_type=\"GRU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ7O5VuIU4qx",
        "outputId": "4013110a-be32-4c40-8a5e-12ed84061e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: LSTM\n",
            "Test loss: nan\n",
            "Test accuracy: 0.908\n",
            "Model: GRU\n",
            "Test loss: nan\n",
            "Test accuracy: 0.916\n"
          ]
        }
      ],
      "source": [
        "test(lstm,test_loader,model_type =\"LSTM\")\n",
        "test(gru,test_loader,model_type=\"GRU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zktz-c4a2u4K"
      },
      "source": [
        "#Predicting the sentiment of user data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ghc3gV_CqNf7"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "    # get rid of punctuation\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    # splitting by spaces\n",
        "    test_words = test_text.split()\n",
        "\n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "\n",
        "    return test_ints\n",
        "\n",
        "\n",
        "def predict(net, test_review, sequence_length=200):\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_review)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_input(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    feature_tensor = feature_tensor.to(device)\n",
        "    \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output) \n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response\n",
        "    if(pred.item()==1):\n",
        "        print(\"Positive review detected!\")\n",
        "    else:\n",
        "        print(\"Negative review detected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxPei2VT2s2Q",
        "outputId": "4e4d9db0-f7a3-43a4-ec93-74da5469a9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of model you want to test on GRU\n",
            "Prediction value, pre-rounding: 0.988389\n",
            "Positive review detected!\n"
          ]
        }
      ],
      "source": [
        "test_review = 'This product was  good. I love it.'\n",
        "seq_length=200 # good to use the length that was trained on\n",
        "model_type= input(\"Type of model you want to test on \")\n",
        "if model_type == \"LSTM\":\n",
        "  net = lstm\n",
        "elif (model_type == \"GRU\"):\n",
        "  net = gru\n",
        "\n",
        "predict(net, test_review, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cjbhpwfC5OEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec574d81-b184-4377-8e73-a7abe2aafc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of model you want to test on LSTM\n",
            "Prediction value, pre-rounding: 0.012422\n",
            "Negative review detected.\n"
          ]
        }
      ],
      "source": [
        "test_review = 'terrible'\n",
        "seq_length=200 # good to use the length that was trained on\n",
        "model_type= input(\"Type of model you want to test on \")\n",
        "if model_type == \"LSTM\":\n",
        "  net = lstm\n",
        "elif (model_type == \"GRU\"):\n",
        "  net = gru\n",
        "\n",
        "predict(net, test_review, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noEcS2r-LUvp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}